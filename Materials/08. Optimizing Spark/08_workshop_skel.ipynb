{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop \"Неидеальный код\"\n",
    "\n",
    "Настало время засучить рукава и написать настоящее Spark приложение, а не сидеть в этом вашем юпитере. Вам предстоит работать с уже привычным датасетом ```nbagames.json```, но на этот раз результат должен быть оформлен в виде двух файлов:\n",
    "+ spark-submit скрипт ```run-job.sh```\n",
    "+ ваш код ```app.py```\n",
    "\n",
    "### Условия\n",
    "Представьте, что вы переделываете с нуля приложение, которое до вас написал неопытный Spak инженер. Основной задачей приложения является выполнение ETL и построение ТОП-10 отчета по данным. При этом вам периодически приходят новые данные в формате, соответствующему строкам файла ```nbagames.json```. Неопытный инженер просто сохранял новые данные в оригинальном формате, что делало построение ТОП-10 отчета очень медленным (помимо его кривого кода). В отличие от него, вы решили сохранять данные в иной, оптимальной структуре. Для этого вы решили использовать наиболее распространенный parquet формат. \n",
    "\n",
    "Вам необходимо определить, в какой структуре будут сохранятся новые данные. Например, вы можете сразу \"взрывать\" колонку \"teams\", использовать определенные колонки для партиционирования, или даже сохранять новые данные в несколько разных файлов. Важно одно - на момент приема новых данных у вас нет возможности перестраивать агрегаты по всему датасету, т.к. новые данные могут приходить достаточно часто. \n",
    "\n",
    "Ваше приложение должно работать в двух режимах (выбор режима определяется аргументом в командной строке).\n",
    "\n",
    "Первый режим должен брать все новые данные из папки X и дописывать их в один или несколько parquet файлов в нужной структуре, расположенных в папке Y. X и Y используйте любые удобные вам.\n",
    "\n",
    "Второй режим запускает генерацию ТОП-10 отчета из данных, расположенных в каталоге Y. Алгоритм уже реализован вашим предшественником и представлен ниже в этом ноутбуке. Вам необходимо адаптировать код под новый формат хранения данных в parquet и переписать код, чтобы он работал как можно быстрее, т.к. заказчик просит обновлять данный отчет как можно чаще.\n",
    "\n",
    "К сожалению, перед увольнением инженер решил испортить код и теперь он не работает из-за внесенных синтаксических ошибок\n",
    "\n",
    "P.S. ```run-job.sh``` должен запускаться из консоли и использовать YARN в качестве менеджера ресурсов для Spark.\n",
    "\n",
    "Подсказки:\n",
    "+ Найдите синтаксические ошибки в коде\n",
    "+ Запустите код и посмотрите что он возвращает\n",
    "+ Проанализируйте код и подумайте, какие данные и в каком виде вам нужны для построения отчета\n",
    "+ Не вдавайтесь в смысл отчета - его там не очень много :)\n",
    "+ не пытайтесь повторить все, что написано ниже в коде. Многие вещи там не нужны и будут сбивать вас с толку. Опирайтесь на поставленную задачу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = \"hdfs:///user/atitov/nbagames.json\"\n",
    "parquet_db = \"useless-parquet-file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rdd = sc.textFile(input_dir)\n",
    "var0 = rdd.repartition(1).map(lambda x: json.loads(x)['_id']['$oid']).cache.collect()\n",
    "\n",
    "var1 = rdd.repartition(1).flatMap(lambda x: json.loads(x)['teams']).collect()\n",
    "\n",
    "new_rdd = sc.parallelize(zip(var0, var1))\n",
    "\n",
    "def flatten_json(x):\n",
    "    a = x[1]\n",
    "    a.update({'game_id': x[0]})\n",
    "    return json.dumps(a)\n",
    "\n",
    "new_rdd = new_rdd.map(lambda x: flatten_json(x))\n",
    "\n",
    "df = spark.read.json(new_rdd)\n",
    "\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "players = \\\n",
    "    df.select(col(\"abbreviation\").alias(\"abb\"), explode(col(\"players\"))) \\\n",
    "    .groupBy(col(\"player.player\").alias(\"player\")).agg(collect_set(col(\"abb\")).alias(\"abb\"), count(\"*\").alias(\"count\"))\n",
    "\n",
    "players.coalesce(1).write.mode(\"overwrite\").partitionBy(\"count\").parquet(parquet_db)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, expr\n",
    "\n",
    "players = spark.read.parquet(parquet_db)\n",
    "\n",
    "join_condition = udf(lambda x,y: x in y, BooleanType)\n",
    "result = df.join(players, join_condition(col(\"abbreviation\"), col(\"abb\"))) \\\n",
    "    .filter(col(\"player\").startswith(\"Anthony\"))\n",
    "\n",
    "result.select(col(\"player\"), col(\"abbreviation\"), (col(\"count\") / size(col(\"abb\"))).alias(\"gpt\")) \\\n",
    "    .distinct().groupBy(col(\"abbreviation\")) \\\n",
    "    .agg(\n",
    "        collect_list(col(\"player\")).alias(\"players\"), \n",
    "        size(collect_list(col(\"player\"))).alias(\"size\"), \n",
    "        sum(col(\"gpt\")).alias(\"sum_gpt\")) \\\n",
    "    .orderBy(col(\"size\").desc(), col(\"sum_gpt\").desc).limit(10).show(10, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
